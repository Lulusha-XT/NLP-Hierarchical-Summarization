{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50575bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leuls\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leuls\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\leuls\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your first document text: This repository encompasses a Python implementation for generating document summaries and queries. Leveraging NLTK for basic natural language processing tasks and spaCy for named entity recognition, the code allows users to input text and a style reference. The operational pipeline involves tokenization, stemming, and frequency-based summarization. The resulting summary is collated and analyzed for named entities and sentiment, forming a query. The system is adaptable for diverse use cases, providing a simple yet functional approach to text summarization and query generation.Large Language Models (LLMs) have become integral for various natural language processing tasks. These models, such as GPT-3.5, are capable of understanding and generating human-like text. They are trained on vast amounts of diverse data to capture intricate language patterns and nuances. In this context, their applications range from chatbots and language translation to code generation and content summarization. The capabilities of LLMs continue to advance, leading to new possibilities in the field of artificial intelligence.\n",
      "Enter your second document text: In the realm of natural language processing, Large Language Models (LLMs) play a pivotal role. GPT-3.5, among others, exemplifies the prowess of these models in comprehending and producing text akin to human language. Extensively trained on diverse datasets, LLMs excel at grasping intricate linguistic subtleties. Their utilization spans a spectrum of applications, encompassing chatbot interactions, language translation, code creation, and content summarization. The evolving capabilities of LLMs herald exciting prospects in the realm of artificial intelligence.Python repository leveraging NLTK and spaCy for text summarization and query generation. Input text and style reference trigger tokenization, stemming, and frequency-based summarization. The summary is analyzed for named entities and sentiment, forming a concise query. An adaptable solution for text summarization and query generation\n",
      "\n",
      "Final Summary saved to 'final_summary.txt'.\n",
      " realm natur languag process larg languag model llm play pivot role ong exemplifi prowessmodel comprehend produc texakin human languag extenstrain diver dataset llm excel grasp intric linguist subtleti utilizat span spectrumappl encompass chatbot interact languag translat code creationcont summariz evolv capabl llmherald excit prospectrealm artifici intlligencpythonpositorileverag nltkspacitextsummarizationnd querigener nput textstilefer triggrkenizatistemmingfrequbasesummarizationsummarianalyzednam entitiendsentimentformingconci querydaptableolutionfortextumrizatindquerygenerion\n",
      "\n",
      "Generated Query: realm languag process larg languag model llm play pivot role exemplifi prowessmodel comprehend produc texakin human languag extenstrain diver dataset llm grasp intric linguist subtleti utilizat span spectrumappl encompass chatbot interact languag translat code creationcont summariz evolv capabl llmherald excit prospectrealm artifici intlligencpythonpositorileverag nltkspacitextsummarizationnd querigener textstilefer triggrkenizatistemmingfrequbasesummarizationsummarianalyzednam entitiendsentimentformingconci querydaptableolutionfortextumrizatindquerygenerion\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Remove stopwords, perform stemming\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        words = [stemmer.stem(word.lower()) for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "        preprocessed_sentences.append(\" \".join(words))\n",
    "\n",
    "    return preprocessed_sentences\n",
    "\n",
    "def generate_summary(document, target_length):\n",
    "    # Tokenize and preprocess the document\n",
    "    text_sentences = preprocess_text(document)\n",
    "\n",
    "    # Initialize an empty summary\n",
    "    summary = []\n",
    "\n",
    "    # Keep track of the total length\n",
    "    current_length = 0\n",
    "\n",
    "    # Iterate through sentences and add to summary until target_length is reached\n",
    "    for sentence in text_sentences:\n",
    "        if current_length + len(sentence.split()) <= target_length:\n",
    "            summary.append(sentence)\n",
    "            current_length += len(sentence.split())\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \" \".join(summary)\n",
    "\n",
    "def generate_query(summary):\n",
    "    # Tokenize and tag parts of speech\n",
    "    tokens = word_tokenize(summary)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Extract nouns and adjectives for the query\n",
    "    query_terms = [word for word, pos in pos_tags if pos.startswith('NN') or pos.startswith('JJ')]\n",
    "\n",
    "    # Form a query by joining the extracted terms\n",
    "    query = \" \".join(query_terms)\n",
    "\n",
    "    return query\n",
    "\n",
    "def hierarchical_summarization_with_query(first_doc, second_doc):\n",
    "    # Measure the length of the two documents\n",
    "    len_first_doc = len(first_doc.split())\n",
    "    len_second_doc = len(second_doc.split())\n",
    "\n",
    "    # Compute target lengths proportionally\n",
    "    target_len_first_doc = int(len_first_doc / (len_first_doc + len_second_doc) * 128)  # Assuming 128 Mb context window limit\n",
    "    target_len_second_doc = 128 - target_len_first_doc\n",
    "\n",
    "    # Initialize an empty collated summary\n",
    "    collated_summary = \"\"\n",
    "\n",
    "    # Slice and summarize the second document iteratively\n",
    "    while len(collated_summary.split()) <= target_len_second_doc:\n",
    "        # Slice the second document from start to a point within the context window\n",
    "        sliced_text = second_doc[:target_len_second_doc - len(collated_summary.split())]\n",
    "\n",
    "        # Summarize the slice with no request for the size of the target\n",
    "        summary = generate_summary(sliced_text, target_len_second_doc)\n",
    "\n",
    "        # Add the summary to collated_summary\n",
    "        collated_summary += summary\n",
    "\n",
    "        # Update second_doc by removing the processed part\n",
    "        second_doc = second_doc[len(sliced_text):]\n",
    "\n",
    "        if not second_doc:\n",
    "            break  # Stop if the entire document has been processed\n",
    "\n",
    "    # Repeat shrinking activities until the summary size is within the context window\n",
    "    final_summary = generate_summary(collated_summary, 128)  # Assuming 128 Mb context window limit\n",
    "\n",
    "    # Save the document\n",
    "    with open(\"final_summary.txt\", \"w\") as file:\n",
    "        file.write(final_summary)\n",
    "\n",
    "    # Generate a query from the final summary\n",
    "    generated_query = generate_query(final_summary)\n",
    "\n",
    "    return final_summary, generated_query\n",
    "\n",
    "# Input prompts\n",
    "user_first_doc = input(\"Enter your first document text: \")\n",
    "user_second_doc = input(\"Enter your second document text: \")\n",
    "\n",
    "result_summary, generated_query = hierarchical_summarization_with_query(user_first_doc, user_second_doc)\n",
    "\n",
    "print(\"\\nFinal Summary saved to 'final_summary.txt'.\\n\", result_summary)\n",
    "print(\"\\nGenerated Query:\", generated_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b9cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
